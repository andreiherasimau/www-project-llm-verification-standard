# Appendix A: Glossary

* **Large Language Model (LLM)** – A type of artificial intelligence model designed to understand, generate, and interact with human language, based on vast amounts of text data. LLMs can perform a variety of language tasks like translation, summarization, and question answering.

* **Prompt Injection** – A technique where an attacker intentionally crafts inputs (or "prompts") to manipulate or exploit the behavior of an LLM. This can involve inserting misleading, biased, or malicious information in a prompt to influence the model's output.

* **LLM Agent** – A software entity or bot that utilizes a Large Language Model to perform tasks, answer queries, or interact in conversations, often designed to automate certain functions or provide user assistance.

* **Model Poisoning** – A malicious attempt to influence or corrupt a machine learning model's training data, causing it to learn incorrect, biased, or harmful behaviors.

* **Natural Language Processing (NLP)** – The field of computer science and artificial intelligence focused on enabling computers to understand, interpret, and generate human language.

* **Transformer Architecture** – A neural network architecture used in many modern LLMs. It is known for its ability to handle sequential data and its effectiveness in tasks involving natural language.

* **Tokenization** – The process of converting text into smaller units (tokens), such as words, characters, or subwords, which can be used as input for language models.

* **Fine-Tuning** – The process of taking a pre-trained model and further training it on a specific dataset to specialize it for particular tasks or domains.

* **Data Privacy** – Concerns related to the handling, processing, and storage of sensitive or personal information by language models, especially when dealing with user inputs.

* **Bias in AI** – The phenomenon where AI models, including LLMs, exhibit biased behavior, often as a result of biased training data or algorithms.

* **Adversarial Attack** – A strategy where attackers create inputs to deceive AI models into making errors. This is particularly concerning in security-sensitive applications of LLMs.

* **Principle of Least Privilege** – A security concept that involves granting users or systems the minimal level of access or permissions necessary to perform their tasks. This principle helps minimize potential damage from accidents or malicious attacks by limiting access rights for users to the bare minimum necessary to complete their duties.
